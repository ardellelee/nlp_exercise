{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a knowledge extraction exercise on hypernym hyponym relationships, including:\n",
    "1. extracting similar words from a corpus by using word embeddings (word2vec or glove)\n",
    "2. Doc2vec /paragraph2vec\n",
    "3. extract hyponyms and hypernym relationships from a given corpus\n",
    "\n",
    "# 1. Similar words extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bromwell', 'high', 'is', 'a', 'cartoon', 'comedy', 'it', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life', 'such', 'as', 'teachers', 'my', '35', 'years', 'in', 'the', 'teaching', 'profession', 'lead', 'me', 'to', 'believe', 'that', 'bromwell', 'highs', 'satire', 'is', 'much', 'closer', 'to', 'reality', 'than', 'is', 'teachers', 'the', 'scramble', 'to', 'survive', 'financially', 'the', 'insightful', 'students', 'who', 'can', 'see', 'right', 'through', 'their', 'pathetic', 'teachers', 'pomp', 'the', 'pettiness', 'of', 'the', 'whole', 'situation', 'all', 'remind', 'me', 'of', 'the', 'schools', 'i', 'knew', 'and', 'their', 'students', 'when', 'i', 'saw', 'the', 'episode', 'in', 'which', 'a', 'student', 'repeatedly', 'tried', 'to', 'burn', 'down', 'the', 'school', 'i', 'immediately', 'recalled', '', 'at', '', 'high', 'a', 'classic', 'line', 'inspector', 'im', 'here', 'to', 'sack', 'one', 'of', 'your', 'teachers', 'student', 'welcome', 'to', 'bromwell', 'high', 'i', 'expect', 'that', 'many', 'adults', 'of', 'my', 'age', 'think', 'that', 'bromwell', 'high', 'is', 'far', 'fetched', 'what', 'a', 'pity', 'that', 'it', 'isnt']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import gensim\n",
    "from gensim import models\n",
    "\n",
    "# read files and split to words\n",
    "f = '/Users/ardellelee/Desktop/data/0_9.txt'\n",
    "for line in open(f):\n",
    "    texts = [text for text in line.lower().split()]  \n",
    "    texts = [text.translate(None, string.punctuation) for text in texts] \n",
    "print texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=92, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "# word2vec model\n",
    "model = gensim.models.Word2Vec([texts], size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "model.save(\"model.word2vec\")\n",
    "print model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3.37865879e-03,   3.28831701e-03,  -4.18940932e-03,\n",
       "        -1.89961749e-03,   4.70168842e-03,  -3.95148853e-03,\n",
       "         3.46579473e-03,  -2.31435965e-03,   3.93820321e-03,\n",
       "         4.91171086e-04,  -4.82577085e-03,   2.73551396e-03,\n",
       "         2.42006197e-03,  -2.79706647e-03,  -3.55122727e-03,\n",
       "         3.63100530e-03,   4.54002619e-03,  -6.29983377e-04,\n",
       "        -4.85184556e-03,  -1.78805611e-04,   2.57958705e-03,\n",
       "         2.33917590e-03,  -2.50906777e-03,   5.14900603e-04,\n",
       "        -2.14807293e-03,  -3.71929398e-03,  -1.84945238e-05,\n",
       "         1.28205720e-04,   1.34782703e-03,   4.99846973e-03,\n",
       "         2.00031558e-03,   9.98484553e-04,  -3.55046033e-03,\n",
       "         3.64268664e-03,   3.32134753e-03,   1.34688686e-04,\n",
       "         1.81385665e-03,  -8.16292537e-04,  -3.13478312e-03,\n",
       "        -2.96940887e-03,   2.77054403e-03,  -4.47312108e-04,\n",
       "        -4.56487481e-03,   1.73767935e-03,  -3.15780053e-04,\n",
       "        -1.18532008e-03,  -3.51529522e-03,   1.00721989e-03,\n",
       "         3.65701481e-03,  -3.26777808e-04,  -4.25059674e-03,\n",
       "        -3.53353284e-03,  -2.83674011e-03,  -3.68555752e-03,\n",
       "         1.42589328e-03,  -4.32424666e-03,  -4.47693421e-03,\n",
       "        -8.80106090e-05,   3.66611959e-04,  -1.28088647e-03,\n",
       "         1.54962041e-03,  -2.38993810e-03,  -4.65118745e-03,\n",
       "        -5.01509849e-03,   2.62450264e-03,   1.14771107e-03,\n",
       "         1.12682476e-03,   3.87052866e-03,  -2.41046585e-03,\n",
       "        -1.87334162e-03,  -1.86201651e-03,   4.99809813e-03,\n",
       "         2.47089285e-03,   3.91777093e-03,  -4.47666412e-03,\n",
       "        -4.14847862e-04,  -4.22344683e-03,   6.31877687e-04,\n",
       "        -3.41493171e-03,   5.96009369e-04,   4.41381382e-03,\n",
       "        -2.77922116e-03,   1.96749228e-03,   3.61148239e-04,\n",
       "         1.21546362e-03,  -1.37091032e-03,   4.50963737e-04,\n",
       "        -2.29440324e-04,  -2.02099280e-03,  -2.64003919e-03,\n",
       "         4.08349978e-03,  -3.15654348e-03,  -1.49908743e-03,\n",
       "        -2.20445963e-03,   1.69154804e-03,  -2.34958855e-03,\n",
       "         2.45977193e-04,   9.63399361e-04,  -1.78236305e-03,\n",
       "         7.77626818e-04], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check model\n",
    "model['bromwell']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.114629263620314"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check similarity of 2 words\n",
    "model.similarity('teachers', 'students')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('whole', 0.23792517185211182),\n",
       " ('one', 0.1994488388299942),\n",
       " ('than', 0.19796141982078552),\n",
       " ('classic', 0.18242147564888),\n",
       " ('think', 0.16298896074295044),\n",
       " ('far', 0.15710151195526123),\n",
       " ('schools', 0.15505558252334595),\n",
       " ('me', 0.14190302789211273),\n",
       " ('such', 0.14071935415267944),\n",
       " ('time', 0.14030654728412628)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find similar words \n",
    "model.wv.most_similar(positive=['teachers', 'students'], negative=['students'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.  Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "['0_9.txt', '10_9.txt', '11_9.txt', '12_9.txt', '13_7.txt', '14_10.txt', '15_7.txt', '16_7.txt', '17_9.txt', '18_7.txt', '19_10.txt', '1_7.txt', '2_9.txt', '3_10.txt', '4_8.txt', '5_10.txt', '6_10.txt', '7_7.txt', '8_7.txt', '9_7.txt']\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "## Read files. Use file name as original label\n",
    "\n",
    "myDirPath = \"/Users/ardellelee/Desktop/data\"\n",
    "\n",
    "docLabels = []\n",
    "docLabels = [f for f in listdir(myDirPath) if f.endswith('.txt')]\n",
    "\n",
    "print len(docLabels)\n",
    "print docLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LabeledSentence(['liked', 'stanley', '', 'iris', 'very', 'much', 'acting', 'was', 'very', 'good', 'story', 'had', 'a', 'unique', 'and', 'interesting', 'arrangement', 'the', 'absence', 'of', 'violence', 'and', 'porno', 'sex', 'was', 'refreshing', 'characters', 'were', 'very', 'convincing', 'and', 'felt', 'like', 'you', 'could', 'understand', 'their', 'feelings', 'very', 'enjoyable', 'movie'], ['DOC_11_9.txt'])\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# Read file contents\n",
    "\n",
    "docs = []\n",
    "for label in docLabels:\n",
    "    text = open(myDirPath+\"/\"+label,'r').read()\n",
    "    words = [word for word in text.lower().split()]\n",
    "    words = [word.translate(None, string.punctuation) for word in words]\n",
    "    doc = gensim.models.doc2vec.LabeledSentence(words=words, tags=['DOC_%s' % label])\n",
    "    docs.append(doc)\n",
    "print docs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DOC_0_9.txt']\n",
      "['DOC_10_9.txt']\n",
      "['DOC_11_9.txt']\n",
      "['DOC_12_9.txt']\n",
      "['DOC_13_7.txt']\n",
      "['DOC_14_10.txt']\n",
      "['DOC_15_7.txt']\n",
      "['DOC_16_7.txt']\n",
      "['DOC_17_9.txt']\n",
      "['DOC_18_7.txt']\n",
      "['DOC_19_10.txt']\n",
      "['DOC_1_7.txt']\n",
      "['DOC_2_9.txt']\n",
      "['DOC_3_10.txt']\n",
      "['DOC_4_8.txt']\n",
      "['DOC_5_10.txt']\n",
      "['DOC_6_10.txt']\n",
      "['DOC_7_7.txt']\n",
      "['DOC_8_7.txt']\n",
      "['DOC_9_7.txt']\n"
     ]
    }
   ],
   "source": [
    "# new labels\n",
    "for doc in docs:\n",
    "    print doc.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# doc2vec model\n",
    "model1 = gensim.models.Doc2Vec(size=300, window=10, min_count=5, workers=11,alpha=0.025, min_alpha=0.025)\n",
    "model1.build_vocab(docs)\n",
    "\n",
    "model.save(\"model1.doc2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('DOC_19_10.txt', 0.11412099003791809), ('DOC_15_7.txt', 0.10329915583133698), ('DOC_8_7.txt', 0.0888400748372078), ('DOC_17_9.txt', 0.06455331295728683), ('DOC_1_7.txt', 0.03995528072118759), ('DOC_16_7.txt', 0.03599502146244049), ('DOC_14_10.txt', 0.03135249391198158), ('DOC_11_9.txt', 0.012747175991535187), ('DOC_18_7.txt', 0.010419502854347229), ('DOC_6_10.txt', 0.0017809867858886719)]\n"
     ]
    }
   ],
   "source": [
    "# check model\n",
    "print model1.docvecs.most_similar('DOC_0_9.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.  Hyponym-hyperny detection - spaCy\n",
    "With reference to a paper - [Automatic Acquisition of Hyponyms\n",
    "from Large Text Corpora](http://people.ischool.berkeley.edu/~hearst/papers/coling92.pdf), hypo-hyper relationship is implied by some lexico-syntactic patterns. \n",
    "\n",
    "\n",
    "Pattern1: NP0{,} such as {NP1, NP2 ... , (and|or)} NPn\n",
    "\n",
    "\n",
    "Pattern2: such NP as {NP ,} * {(or | and)} NP\n",
    "\n",
    "\n",
    "Pattern3: NP {, NP} * {,} or other NP\n",
    "\n",
    "\n",
    "Pattern4: NP {, NP} * {,} and other NP\n",
    "\n",
    "\n",
    "Pattern5: NP {,} including {NP ,} * {or | and} NP\n",
    "\n",
    "\n",
    "Pattern6: NP {,} especially {NP ,} * {or | and} NP\n",
    "\n",
    "\n",
    "\n",
    "We firstly extract the text chunk which matches the pattern, and then extract nouns and organize the hypo-hyper pair.\n",
    "\n",
    "\n",
    "In Part3, we implement the method using spaCy. In Part4, we will try the implementation with another python nlp package - NLTK.\n",
    "\n",
    "\n",
    "\n",
    "The implementation in spaCy bases on a rule-based matcher. Not generalize well since regex is not supported in matcher so patterns are not flexible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.attrs import POS, LOWER, ORTH\n",
    "from spacy.parts_of_speech import NOUN\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "matcher=Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pattern1: NP0{,} such as {NP1, NP2 ... , (and|or)} NPn\n",
    "matcher.add_pattern(\"pattern1\",\n",
    "                    [{POS: \"NOUN\"}, {POS: \"PUNCT\", 'OP': '*'}, {LOWER: \"such\"}, {LOWER:\"as\"}, {POS:\"NOUN\"}])\n",
    "\n",
    "# Pattern2: such NP as {NP ,} *{(or|and)} NP\n",
    "matcher.add_pattern(\"pattern2\",\n",
    "                    [{LOWER: \"such\"}, {POS: \"NOUN\"}, {LOWER: \"as\"},\n",
    "                     {POS:\"PROPN\", \"OP\":\"*\"}, {POS:\"PUNCT\", \"OP\":\"?\"},\n",
    "                     {LOWER: \"or\", \"OP\":\"?\"}, {LOWER: \"and\", \"OP\":\"?\"},{POS: \"PROPN\"}])\n",
    "# Pattern3: NP {, NP} * {,} or other NP\n",
    "matcher.add_pattern(\"pattern3\", [{POS: \"ADJ\", \"OP\":\"*\"},{POS: \"NOUN\"}, {POS:\"PUNCT\"},\n",
    "                                 {POS: \"ADJ\", \"OP\":\"*\"},{POS:\"NOUN\"}, {POS:\"PUNCT\"},\n",
    "                                 {LOWER: \"or\"}, {LOWER: \"other\"},{POS: \"ADJ\", \"OP\":\"*\"}, {POS: \"NOUN\"}])\n",
    "# Pattern4: NP {, NP} * {,} and other NP\n",
    "matcher.add_pattern(\"pattern4\", [{POS: \"ADJ\", \"OP\":\"*\"},{POS: \"NOUN\",\"OP\":\"?\" }, {POS:\"PUNCT\", \"OP\":\"?\"},\n",
    "                                 {POS: \"ADJ\", \"OP\":\"*\"},{POS:\"NOUN\"}, {POS:\"PUNCT\", \"OP\":\"?\"},\n",
    "                                 {LOWER: \"and\"}, {LOWER: \"other\"},{POS: \"ADJ\", \"OP\":\"*\"}, {POS: \"NOUN\"}])\n",
    "# Pattern5: NP {,} including {NP ,} * {or|and} NP\n",
    "matcher.add_pattern(\"pattern5\", [{POS: \"ADJ\", \"OP\":\"*\"},{POS: \"NOUN\"}, {POS:\"PUNCT\", \"OP\":\"?\"},{LOWER: \"including\"},\n",
    "                                 {POS: \"ADJ\", \"OP\":\"*\"},{POS:\"PROPN\"}, {POS:\"PUNCT\", \"OP\":\"?\"},\n",
    "                                 {LOWER: \"and\", \"OP\":\"?\"},{POS: \"ADJ\", \"OP\":\"*\"}, {POS: \"PROPN\"}])\n",
    "\n",
    "#Pattern6: NP {,} especially {NP ,}* {or|and} NP\n",
    "matcher.add_pattern(\"pattern6\", [{POS: \"NOUN\"}, {POS:\"PUNCT\", \"OP\":\"?\"},{LOWER: \"especially\"}, \n",
    "                                 {POS: \"ADJ\", \"OP\":\"*\"},{POS:\"PROPN\", \"OP\":\"?\"}, {POS:\"PUNCT\", \"OP\":\"?\"},\n",
    "                                 {LOWER:\"and\"}, {POS: \"ADJ\", \"OP\":\"*\"}, {POS: \"PROPN\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testcases=[\n",
    "    u'I want a tool, such as zimmer',\n",
    "    u'works by such authors as Herrick, Goldsmith, and Shakespeare',\n",
    "    u'Bruises, wounds or other injuries ...',\n",
    "    u'... temples, treasuries,and other important civic buildings',\n",
    "    u'All common-law countries, including Canada and England ...',\n",
    "    u'... most European countries, especially France, England, and Spain.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(..., u'PUNCT'), (most, u'ADJ'), (European, u'ADJ'), (countries, u'NOUN'), (,, u'PUNCT'), (especially, u'ADV'), (France, u'PROPN'), (,, u'PUNCT'), (England, u'PROPN'), (,, u'PUNCT'), (and, u'CCONJ'), (Spain, u'PROPN'), (., u'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "'''doc = nlp(testcases[5])\n",
    "print([(t, t.pos_) for t in doc])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most European countries, especially France, England\n",
      "most European countries, especially France, England\n",
      "European countries, especially France, England\n",
      "European countries, especially France, England\n",
      "countries, especially France, England\n",
      "countries, especially France, England\n"
     ]
    }
   ],
   "source": [
    "'''for ent_id, label, start, end in matcher(doc):\n",
    "    span=doc[start:end]\n",
    "    print (span)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test case by case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pattern extracted:  [most, European, countries, ,, especially, France, ,, England]\n",
      "hypo list:  [France, England]\n",
      "hyponymy(France,most)\n",
      "hyponymy(England,most)\n",
      "pattern extracted:  [most, European, countries, ,, especially, France, ,, England]\n",
      "hypo list:  [France, England]\n",
      "hyponymy(France,most)\n",
      "hyponymy(England,most)\n",
      "pattern extracted:  [European, countries, ,, especially, France, ,, England]\n",
      "hypo list:  [France, England]\n",
      "hyponymy(France,European)\n",
      "hyponymy(England,European)\n",
      "pattern extracted:  [European, countries, ,, especially, France, ,, England]\n",
      "hypo list:  [France, England]\n",
      "hyponymy(France,European)\n",
      "hyponymy(England,European)\n",
      "pattern extracted:  [countries, ,, especially, France, ,, England]\n",
      "hypo list:  [France, England]\n",
      "hyponymy(France,countries)\n",
      "hyponymy(England,countries)\n",
      "pattern extracted:  [countries, ,, especially, France, ,, England]\n",
      "hypo list:  [France, England]\n",
      "hyponymy(France,countries)\n",
      "hyponymy(England,countries)\n"
     ]
    }
   ],
   "source": [
    "# testcase[5]\n",
    "\n",
    "for ent_id, label, start, end in matcher(nlp(testcases[5])):\n",
    "    span=list(doc[start:end])\n",
    "    print 'pattern extracted: ', span\n",
    "    \n",
    "    hypernym=span[0]\n",
    "    hyponym=span[-1]\n",
    "    hypo_list = [c for c in span if (c.pos == hyponym.pos) & (c!=hypernym)]\n",
    "    print 'hypo list: ', hypo_list\n",
    "    \n",
    "    for hypo in hypo_list:\n",
    "        print(\"hyponymy({},{})\".format(hypo, hypernym))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pattern extracted:  [countries, ,, including, Canada, and, England]\n",
      "hypo list:  [Canada, England]\n",
      "hyponymy(Canada,countries)\n",
      "hyponymy(England,countries)\n",
      "pattern extracted:  [countries, ,, including, Canada, and, England]\n",
      "hypo list:  [Canada, England]\n",
      "hyponymy(Canada,countries)\n",
      "hyponymy(England,countries)\n",
      "pattern extracted:  [countries, ,, including, Canada, and, England]\n",
      "hypo list:  [Canada, England]\n",
      "hyponymy(Canada,countries)\n",
      "hyponymy(England,countries)\n",
      "pattern extracted:  [countries, ,, including, Canada, and, England]\n",
      "hypo list:  [Canada, England]\n",
      "hyponymy(Canada,countries)\n",
      "hyponymy(England,countries)\n",
      "pattern extracted:  [countries, ,, including, Canada, and, England]\n",
      "hypo list:  [Canada, England]\n",
      "hyponymy(Canada,countries)\n",
      "hyponymy(England,countries)\n",
      "pattern extracted:  [countries, ,, including, Canada, and, England]\n",
      "hypo list:  [Canada, England]\n",
      "hyponymy(Canada,countries)\n",
      "hyponymy(England,countries)\n",
      "pattern extracted:  [countries, ,, including, Canada, and, England]\n",
      "hypo list:  [Canada, England]\n",
      "hyponymy(Canada,countries)\n",
      "hyponymy(England,countries)\n"
     ]
    }
   ],
   "source": [
    "# testcase[4]\n",
    "\n",
    "for ent_id, label, start, end in matcher(nlp(testcases[4])):\n",
    "    span=list(doc[start:end])\n",
    "    print 'pattern extracted: ', span\n",
    "    \n",
    "    hypernym=span[0]\n",
    "    hyponym=span[-1]\n",
    "    hypo_list = [c for c in span if (c.pos == hyponym.pos) & (c!=hypernym)]\n",
    "    print 'hypo list: ', hypo_list\n",
    "    \n",
    "    for hypo in hypo_list:\n",
    "        print(\"hyponymy({},{})\".format(hypo, hypernym))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# testcase[3]\n",
    "\n",
    "for ent_id, label, start, end in matcher(nlp(testcases[3])):\n",
    "    span=list(doc[start:end])\n",
    "    print 'pattern extracted: ', span\n",
    "    \n",
    "    hypernym=span[-1]\n",
    "    #hyponym=span[-1]\n",
    "    hypo_list = [c for c in span if (c.pos == hypernym.pos) & (c!=hypernym)]\n",
    "    print 'hypo list: ', hypo_list\n",
    "    \n",
    "    for hypo in hypo_list:\n",
    "        print(\"hyponymy({},{})\".format(hypo, hypernym))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pattern extracted:  [Bruises, ,, wounds, or, other, injuries]\n",
      "hypo list:  [Bruises, wounds]\n",
      "hyponymy(Bruises,injuries)\n",
      "hyponymy(wounds,injuries)\n",
      "pattern extracted:  [Bruises, ,, wounds, or, other, injuries]\n",
      "hypo list:  [Bruises, wounds]\n",
      "hyponymy(Bruises,injuries)\n",
      "hyponymy(wounds,injuries)\n",
      "pattern extracted:  [Bruises, ,, wounds, or, other, injuries]\n",
      "hypo list:  [Bruises, wounds]\n",
      "hyponymy(Bruises,injuries)\n",
      "hyponymy(wounds,injuries)\n",
      "pattern extracted:  [Bruises, ,, wounds, or, other, injuries]\n",
      "hypo list:  [Bruises, wounds]\n",
      "hyponymy(Bruises,injuries)\n",
      "hyponymy(wounds,injuries)\n",
      "pattern extracted:  [Bruises, ,, wounds, or, other, injuries]\n",
      "hypo list:  [Bruises, wounds]\n",
      "hyponymy(Bruises,injuries)\n",
      "hyponymy(wounds,injuries)\n",
      "pattern extracted:  [Bruises, ,, wounds, or, other, injuries]\n",
      "hypo list:  [Bruises, wounds]\n",
      "hyponymy(Bruises,injuries)\n",
      "hyponymy(wounds,injuries)\n",
      "pattern extracted:  [,, wounds, or, other, injuries]\n",
      "hypo list:  [wounds]\n",
      "hyponymy(wounds,injuries)\n",
      "pattern extracted:  [,, wounds, or, other, injuries]\n",
      "hypo list:  [wounds]\n",
      "hyponymy(wounds,injuries)\n",
      "pattern extracted:  [,, wounds, or, other, injuries]\n",
      "hypo list:  [wounds]\n",
      "hyponymy(wounds,injuries)\n",
      "pattern extracted:  [,, wounds, or, other, injuries]\n",
      "hypo list:  [wounds]\n",
      "hyponymy(wounds,injuries)\n",
      "pattern extracted:  [,, wounds, or, other, injuries]\n",
      "hypo list:  [wounds]\n",
      "hyponymy(wounds,injuries)\n",
      "pattern extracted:  [wounds, or, other, injuries]\n",
      "hypo list:  [wounds]\n",
      "hyponymy(wounds,injuries)\n"
     ]
    }
   ],
   "source": [
    "# testcase[2]\n",
    "\n",
    "for ent_id, label, start, end in matcher(nlp(testcases[2])):\n",
    "    span=list(doc[start:end])\n",
    "    print 'pattern extracted: ', span\n",
    "    \n",
    "    hypernym=span[-1]\n",
    "    #hyponym=span[-1]\n",
    "    hypo_list = [c for c in span if (c.pos == hypernym.pos) & (c!=hypernym)]\n",
    "    print 'hypo list: ', hypo_list\n",
    "    \n",
    "    for hypo in hypo_list:\n",
    "        print(\"hyponymy({},{})\".format(hypo, hypernym))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pattern extracted:  [such, authors, as, Herrick, ,, Goldsmith]\n",
      "hypo list:  [Herrick, Goldsmith]\n",
      "hyponymy(Herrick,authors)\n",
      "hyponymy(Goldsmith,authors)\n"
     ]
    }
   ],
   "source": [
    "# testcase[1]\n",
    "for ent_id, label, start, end in matcher(nlp(testcases[1])):\n",
    "    span=list(doc[start:end])\n",
    "    print 'pattern extracted: ', span\n",
    "    \n",
    "    hypernym=span[1]\n",
    "    hyponym=span[-1]\n",
    "    hypo_list = [c for c in span if (c.pos == hyponym.pos) & (c!=hypernym)]\n",
    "    print 'hypo list: ', hypo_list\n",
    "    \n",
    "    for hypo in hypo_list:\n",
    "        print(\"hyponymy({},{})\".format(hypo, hypernym))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pattern extracted:  [tool, ,, such, as, zimmer]\n",
      "hypo list:  [zimmer]\n",
      "hyponymy(zimmer,tool)\n"
     ]
    }
   ],
   "source": [
    "# testcase[0]\n",
    "for ent_id, label, start, end in matcher(nlp(testcases[0])):\n",
    "    span=list(doc[start:end])\n",
    "    print 'pattern extracted: ', span\n",
    "    \n",
    "    hypernym=span[0]\n",
    "    hyponym=span[-1]\n",
    "    hypo_list = [c for c in span if (c.pos == hyponym.pos) & (c!=hypernym)]\n",
    "    print 'hypo list: ', hypo_list\n",
    "    \n",
    "    for hypo in hypo_list:\n",
    "        print(\"hyponymy({},{})\".format(hypo, hypernym))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Hyponym-hyperny detection\n",
    "An implementation in nltk with RegexpParser. Patterns are defined with regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import RegexpParser\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "\n",
    "testcases=[\n",
    "    u'I want to buy a gift, such as cake, flower, or CD ',\n",
    "    u'works by such authors as Herrick, Goldsmith, Shakespeare',\n",
    "    u'Bruises, wounds or other injuries ...',\n",
    "    u'... temples, treasuries,and other important civic buildings',\n",
    "    u'All common-law countries, including Canada and England ...',\n",
    "    u'... most European countries, especially France, England, and Spain.'\n",
    "]\n",
    "\n",
    "pos_list = [pos_tag(word_tokenize(testcase), tagset='universal') for testcase in testcases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Pattern1: NP0 such as {NP1, NP2 ... , (and|or)} NPn\n",
    "Pattern2: such NP as {NP ,} * {(or|and)} NP\n",
    "Pattern3: NP {, NP} * {,} or other NP\n",
    "Pattern4: NP {, NP} * {,} and other NP\n",
    "Pattern5: NP {,} including {NP ,} * {or|and} NP\n",
    "Pattern6: NP {,} especially {NP ,}* {or|and} NP\n",
    "'''\n",
    "\n",
    "grammar = r\"\"\"\n",
    "  P1: {<NOUN><.>?<ADJ><ADP>(<DET>?<NOUN><.>?)+}\n",
    "  P2: {<ADJ><NOUN><ADP>(<NOUN><.>)+<NOUN>}\n",
    "  P3: {<NOUN>(<.>?<NOUN>)+<.>?<CONJ><ADJ>+<NOUN>}\n",
    "  P4: {<NOUN>(<.>?<NOUN>)+<.>?<CONJ><ADJ>+<NOUN>}     #P4 is the same as P3\n",
    "  P5: {<NOUN><.>?<VERB><NOUN><.>?<CONJ>?<NOUN>}\n",
    "  P6: {<NOUN><.>?<ADV>(<NOUN><.>)+<CONJ>?<NOUN>}\n",
    "\"\"\"\n",
    "\n",
    "cp = RegexpParser(grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test case by case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted patterns:  gift , such as cake , flower ,\n",
      "Hypo-hyper pairs:  [(u'cake', u'gift'), (u'flower', u'gift')]\n"
     ]
    }
   ],
   "source": [
    "# testcase[0]\n",
    "'''\n",
    "u'I want to buy a gift, such as cake, flower, or CD '\n",
    " P1: {<NOUN><.>?<ADJ><ADP>(<DET>?<NOUN><.>?)+}\n",
    "'''\n",
    "\n",
    "tree=cp.parse(pos_list[0])\n",
    "for sub in tree.subtrees():\n",
    "    if sub.label()=='P1':\n",
    "        hyper_hypo=[t[0] for t in sub.leaves() if t[1]=='NOUN']\n",
    "        str1=' '.join([t[0] for t in sub.leaves()])\n",
    "        print 'Extracted patterns: ' ,str1\n",
    "        hyper = hyper_hypo[0]\n",
    "        result=[(t, hyper) for t in hyper_hypo if t != hyper]\n",
    "        print 'Hypo-hyper pairs: ', result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted patterns:  such authors as Herrick , Goldsmith , Shakespeare\n",
      "Hypo-hyper pairs:  [(u'Herrick', u'authors'), (u'Goldsmith', u'authors'), (u'Shakespeare', u'authors')]\n"
     ]
    }
   ],
   "source": [
    "# testcase[1]\n",
    "'''\n",
    "u'works by such authors as Herrick, Goldsmith, Shakespeare'\n",
    "P2: {<ADJ><NOUN><ADP>(<NOUN><.>)+<NOUN>}\n",
    "'''\n",
    "\n",
    "tree=cp.parse(pos_list[1])\n",
    "for sub in tree.subtrees():\n",
    "    if sub.label()=='P2':\n",
    "        hyper_hypo=[t[0] for t in sub.leaves() if t[1]=='NOUN']\n",
    "        str1=' '.join([t[0] for t in sub.leaves()])\n",
    "        print 'Extracted patterns: ' ,str1\n",
    "        hyper = hyper_hypo[0]\n",
    "        result=[(t, hyper) for t in hyper_hypo if t != hyper]\n",
    "        print 'Hypo-hyper pairs: ', result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted patterns:  Bruises , wounds or other injuries\n",
      "Hypo-hyper pairs:  [(u'Bruises', u'injuries'), (u'wounds', u'injuries')]\n"
     ]
    }
   ],
   "source": [
    "# testcase[2]\n",
    "'''\n",
    "u'Bruises, wounds or other injuries ...'\n",
    "P3: {<NOUN>(<.>?<NOUN>)+<.>?<CONJ><ADJ>+<NOUN>}\n",
    "'''\n",
    "\n",
    "tree=cp.parse(pos_list[2])\n",
    "for sub in tree.subtrees():\n",
    "    if sub.label()=='P3':\n",
    "        hyper_hypo=[t[0] for t in sub.leaves() if t[1]=='NOUN']\n",
    "        str1=' '.join([t[0] for t in sub.leaves()])\n",
    "        print 'Extracted patterns: ' ,str1\n",
    "        hyper = hyper_hypo[-1]\n",
    "        result=[(t, hyper) for t in hyper_hypo if t != hyper]\n",
    "        print 'Hypo-hyper pairs: ', result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted patterns:  temples , treasuries , and other important civic\n",
      "Hypo-hyper pairs:  [(u'temples', u'civic'), (u'treasuries', u'civic')]\n"
     ]
    }
   ],
   "source": [
    "# testcase[3]\n",
    "'''\n",
    "u'... temples, treasuries,and other important civic buildings'\n",
    "P4: {<NOUN>(<.>?<NOUN>)+<.>?<CONJ><ADJ>+<NOUN>}\n",
    "'''\n",
    "\n",
    "tree=cp.parse(pos_list[3])\n",
    "#print tree\n",
    "for sub in tree.subtrees():\n",
    "    if (sub.label()=='P3')|(sub.label=='P4'):\n",
    "        hyper_hypo=[t[0] for t in sub.leaves() if t[1]=='NOUN']\n",
    "        str1=' '.join([t[0] for t in sub.leaves()])\n",
    "        print 'Extracted patterns: ' ,str1\n",
    "        hyper = hyper_hypo[-1]\n",
    "        result=[(t, hyper) for t in hyper_hypo if t != hyper]\n",
    "        print 'Hypo-hyper pairs: ', result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted patterns:  countries , including Canada and England\n",
      "Hypo-hyper pairs:  [(u'Canada', u'countries'), (u'England', u'countries')]\n"
     ]
    }
   ],
   "source": [
    "# testcase[4]\n",
    "'''\n",
    "u'All common-law countries, including Canada and England ...''\n",
    "Pattern5: NP {,} including {NP ,} * {or|and} NP\n",
    "'''\n",
    "\n",
    "tree=cp.parse(pos_list[4])\n",
    "#print tree\n",
    "for sub in tree.subtrees():\n",
    "    if (sub.label()=='P5'):\n",
    "        hyper_hypo=[t[0] for t in sub.leaves() if t[1]=='NOUN']\n",
    "        str1=' '.join([t[0] for t in sub.leaves()])\n",
    "        print 'Extracted patterns: ' ,str1\n",
    "        hyper = hyper_hypo[0]\n",
    "        result=[(t, hyper) for t in hyper_hypo if t != hyper]\n",
    "        print 'Hypo-hyper pairs: ', result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted patterns:  countries , especially France , England , and Spain\n",
      "Hypo-hyper pairs:  [(u'France', u'countries'), (u'England', u'countries'), (u'Spain', u'countries')]\n"
     ]
    }
   ],
   "source": [
    "# testcase[5]\n",
    "'''\n",
    "u'... most European countries, especially France, England, and Spain.'\n",
    "P6: {<NOUN><.>?<ADV>(<NOUN><.>)+<CONJ>?<NOUN>}\n",
    "'''\n",
    "\n",
    "tree=cp.parse(pos_list[5])\n",
    "#print tree\n",
    "for sub in tree.subtrees():\n",
    "    if (sub.label()=='P6'):\n",
    "        hyper_hypo=[t[0] for t in sub.leaves() if t[1]=='NOUN']\n",
    "        str1=' '.join([t[0] for t in sub.leaves()])\n",
    "        print 'Extracted patterns: ' ,str1\n",
    "        hyper = hyper_hypo[0]\n",
    "        result=[(t, hyper) for t in hyper_hypo if t != hyper]\n",
    "        print 'Hypo-hyper pairs: ', result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
