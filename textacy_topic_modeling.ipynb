{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "textacy version: 0.4.1\n",
      "spacy version: 1.9.0\n"
     ]
    }
   ],
   "source": [
    "import textacy\n",
    "import textacy.datasets\n",
    "import spacy\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "#nlp=spacy.load('en')\n",
    "\n",
    "print 'textacy version:', textacy.__version__\n",
    "print 'spacy version:', spacy.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Corpus(1000 docs; 538150 tokens)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textacy.vsm import Vectorizer\n",
    "from textacy.tm.topic_model import TopicModel\n",
    "\n",
    "# import text stream from textacy datasets\n",
    "cw = textacy.datasets.CapitolWords()\n",
    "\n",
    "text_stream, metadata_stream = textacy.fileio.split_record_fields(cw.records(limit=1000), 'text', itemwise=False)\n",
    "\n",
    "corpus = textacy.Corpus('en', texts=text_stream, metadatas=metadata_stream)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize words and create DTM\n",
    "\n",
    "In textacy, class Vectorizer is used to transform one or more tokenized documents into a document-term matrix.\n",
    "\n",
    "The shape of DTM is (#docs, #unique terms). The weighting method is optional among: tf-, tf-idf, or binary-weighted.\n",
    "\n",
    "\n",
    "The first time we use a vectorizer, we need to initialize it with parameters. The process where term list is transformed into a DTM is also the process of training. If we want to use the same vectorizer, next time we can use .transform() method instead of .fit_transform(). \n",
    "\n",
    "Note: every time do the transform, we need to specify the term list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Every time when vectorizing, need to specify terms-list.\n",
    "terms_list = (doc.to_terms_list(ngrams=1, named_entities=True, as_strings=True) for doc in corpus[:500]) #select first 500 words\n",
    "\n",
    "# initialize a vectorizer\n",
    "vectorizer = Vectorizer(weighting='tfidf', normalize=True, smooth_idf=True, min_df=3, max_df=0.95, max_n_terms=100000)\n",
    "# train the vectorizer as transforming term-lists into DTM\n",
    "doc_term_matrix = vectorizer.fit_transform(terms_list)\n",
    "\n",
    "#print doc_term_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic modeling\n",
    "\n",
    "Topic models in Textacy are built on scikit-learn. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.10034641  0.0253678   0.         ...,  0.          0.04499283\n",
      "   0.00073213]\n",
      " [ 0.07038761  0.          0.         ...,  0.00804994  0.          0.        ]\n",
      " [ 0.13657403  0.00983649  0.         ...,  0.          0.07124204  0.        ]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.02743931  0.10817356  0.06424196 ...,  0.          0.          0.08101567]\n",
      " [ 0.          0.0272248   0.         ...,  0.          0.          0.39016118]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize a topic model\n",
    "model = textacy.tm.TopicModel('nmf', n_topics=10) # n_topics: number of topics\n",
    "\n",
    "# Train model with DTM\n",
    "model.fit(doc_term_matrix)\n",
    "\n",
    "# Use trained model to transform DTM to topics \n",
    "doc_topic_matrix = model.transform(doc_term_matrix)\n",
    "\n",
    "print doc_topic_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('/Users/ardellelee/PycharmProjects/textacy/mymodel_nmf.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Interpretion\n",
    "\n",
    "Input parameter:\n",
    "\n",
    "topics=[0,1,...]/range(7): the list to specify topics/docs\n",
    "\n",
    "top_n: number of outputs for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'topic', 0, u':', u'people   go   want   bill   think   money   $   program   work   year')\n",
      "(u'topic', 5, u':', u'amendment   chairman   clerk   designate   offer   mr.   sanders   withdraw   vermont   gentleman')\n"
     ]
    }
   ],
   "source": [
    "# check top terms occured in specified topic\n",
    "\n",
    "for topic_idx, top_terms in model.top_topic_terms(vectorizer.id_to_term, topics=[0,5], top_n=10): \n",
    "    print('topic', topic_idx, ':', '   '.join(top_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "EMERGENCY RELIEF\n",
      "REBUTTAL TO PRESIDENTIAL SPEECH\n",
      "FOREIGN RELATIONS AUTHORIZATION ACT, FISCAL YEARS 1996 AND 1997-- CONFERENCE REPORT\n",
      "7\n",
      "FOREIGN OPERATIONS, EXPORT FINANCING, AND RELATED PROGRAMS APPROPRIATIONS ACT, 1997\n",
      "RECESS\n",
      "BALANCED BUDGET DOWNPAYMENT ACT, II\n"
     ]
    }
   ],
   "source": [
    "# check the top docs where specified topics occur\n",
    "\n",
    "for topic_idx, top_docs in model.top_topic_docs(doc_topic_matrix, topics=[2,7], top_n=3):\n",
    "    print(topic_idx)\n",
    "    for j in top_docs:\n",
    "        print(corpus[j].metadata['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u\"EXAMINING THE SPEAKER'S UPCOMING TRAVEL SCHEDULE\", u':', (0, 8, 9, 7))\n",
      "(u'FLOODING IN PENNSYLVANIA', u':', (0, 9, 8, 7))\n"
     ]
    }
   ],
   "source": [
    "# check which topics occur in specified doc\n",
    "\n",
    "for doc_idx, topics in model.top_doc_topics(doc_topic_matrix, docs=[3,4], top_n=4):\n",
    "    print(corpus[doc_idx].metadata['title'], ':', topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0.31936436172376825)\n",
      "(1, 0.11837332487112134)\n",
      "(2, 0.097933043951754895)\n",
      "(3, 0.083756427304841438)\n",
      "(4, 0.098375356008242557)\n",
      "(5, 0.070792376456240674)\n",
      "(6, 0.044612691461782955)\n",
      "(7, 0.045166978725944329)\n",
      "(8, 0.057162099048094439)\n",
      "(9, 0.064463340448209144)\n"
     ]
    }
   ],
   "source": [
    "# check weights of each topic\n",
    "\n",
    "for i, val in enumerate(model.topic_weights(doc_topic_matrix)):\n",
    "    print(i, val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x14ec34150>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "model.termite_plot(doc_term_matrix, vectorizer.id_to_term, topics=-1,  n_terms=25, sort_terms_by='seriation')\n",
    "#plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
